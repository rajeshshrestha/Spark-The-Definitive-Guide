{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of Sparks's Toolset\n",
    "&rightarrow; Spark is composed of primitives - the lower-level APIs and the Structured APIs - and then a series of standard libraries for additional functionality\n",
    "![image](Images/3_1.png)\n",
    "&rightarrow; Spark's libraries support a variety of different tasks, from graph analysisi and machine learning to streaming and integrations with a host of computing and storage systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Running production applications with __spark-submit__\n",
    "2. Datasets: type-safe APIs for structured data\n",
    "3. Structured Straming\n",
    "4. Machine learning and advanced analytics\n",
    "5. Resilient Distributed Datasets(RDD): Spark's low level APIs\n",
    "6. Spark R\n",
    "7. The third-party package ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Production Applications\n",
    "+ Spark makes it easy to develop and create big data programs\n",
    "+ It also makes it easy to turn interactive exploration into production applications with __spark-submit__, a built-in command-line tool.\n",
    "+ __Spark-submit__ lets us send application code to a cluster and launch it to execute there.\n",
    "+ Upon submission, the application will run until it exists with completion or encounters an error.\n",
    "+ This can be used with all of Spark's support cluster managers:\n",
    "    + Standalone\n",
    "    + Mesos\n",
    "    + YARN\n",
    "+ __Spark-submit__ offers several controls with which we can specify the resources our application needs as well as how it should be run and its command-line arguments\n",
    "+ __Spark-submit__ can be used with any applications written in any of Spark's supported language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/06/25 00:33:21 WARN Utils: Your hostname, raj-Predator-G3-572 resolves to a loopback address: 127.0.1.1; using 192.168.1.118 instead (on interface enp3s0f1)\n",
      "20/06/25 00:33:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/06/25 00:33:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/06/25 00:33:22 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/06/25 00:33:22 INFO SparkContext: Submitted application: PythonPi\n",
      "20/06/25 00:33:22 INFO SecurityManager: Changing view acls to: raj\n",
      "20/06/25 00:33:22 INFO SecurityManager: Changing modify acls to: raj\n",
      "20/06/25 00:33:22 INFO SecurityManager: Changing view acls groups to: \n",
      "20/06/25 00:33:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/06/25 00:33:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raj); groups with view permissions: Set(); users  with modify permissions: Set(raj); groups with modify permissions: Set()\n",
      "20/06/25 00:33:23 INFO Utils: Successfully started service 'sparkDriver' on port 43243.\n",
      "20/06/25 00:33:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/06/25 00:33:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/06/25 00:33:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/06/25 00:33:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/06/25 00:33:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-67bf85da-cbae-45f0-8af5-c76261eb1734\n",
      "20/06/25 00:33:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/06/25 00:33:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/06/25 00:33:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/06/25 00:33:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.118:4040\n",
      "20/06/25 00:33:23 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/06/25 00:33:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45169.\n",
      "20/06/25 00:33:23 INFO NettyBlockTransferService: Server created on 192.168.1.118:45169\n",
      "20/06/25 00:33:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/06/25 00:33:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.118, 45169, None)\n",
      "20/06/25 00:33:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.118:45169 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.118, 45169, None)\n",
      "20/06/25 00:33:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.118, 45169, None)\n",
      "20/06/25 00:33:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.118, 45169, None)\n",
      "20/06/25 00:33:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/spark-warehouse').\n",
      "20/06/25 00:33:24 INFO SharedState: Warehouse path is 'file:/home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/spark-warehouse'.\n",
      "20/06/25 00:33:24 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/06/25 00:33:24 INFO SparkContext: Starting job: reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43\n",
      "20/06/25 00:33:24 INFO DAGScheduler: Got job 0 (reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43) with 10 output partitions\n",
      "20/06/25 00:33:24 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43)\n",
      "20/06/25 00:33:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/06/25 00:33:24 INFO DAGScheduler: Missing parents: List()\n",
      "20/06/25 00:33:24 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43), which has no missing parents\n",
      "20/06/25 00:33:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.4 KB, free 366.3 MB)\n",
      "20/06/25 00:33:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.3 MB)\n",
      "20/06/25 00:33:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.118:45169 (size: 4.4 KB, free: 366.3 MB)\n",
      "20/06/25 00:33:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163\n",
      "20/06/25 00:33:24 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "20/06/25 00:33:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks\n",
      "20/06/25 00:33:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/06/25 00:33:25 INFO PythonRunner: Times: total = 730, boot = 583, init = 52, finish = 95\n",
      "20/06/25 00:33:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "20/06/25 00:33:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 815 ms on localhost (executor driver) (1/10)\n",
      "20/06/25 00:33:25 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39075\n",
      "20/06/25 00:33:25 INFO PythonRunner: Times: total = 145, boot = 3, init = 46, finish = 96\n",
      "20/06/25 00:33:25 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:25 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:25 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "20/06/25 00:33:25 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 155 ms on localhost (executor driver) (2/10)\n",
      "20/06/25 00:33:26 INFO PythonRunner: Times: total = 143, boot = 3, init = 42, finish = 98\n",
      "20/06/25 00:33:26 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1464 bytes result sent to driver\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 152 ms on localhost (executor driver) (3/10)\n",
      "20/06/25 00:33:26 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "20/06/25 00:33:26 INFO PythonRunner: Times: total = 172, boot = 8, init = 57, finish = 107\n",
      "20/06/25 00:33:26 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:26 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 181 ms on localhost (executor driver) (4/10)\n",
      "20/06/25 00:33:26 INFO PythonRunner: Times: total = 145, boot = 3, init = 44, finish = 98\n",
      "20/06/25 00:33:26 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:26 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 152 ms on localhost (executor driver) (5/10)\n",
      "20/06/25 00:33:26 INFO PythonRunner: Times: total = 150, boot = 3, init = 45, finish = 102\n",
      "20/06/25 00:33:26 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1464 bytes result sent to driver\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 157 ms on localhost (executor driver) (6/10)\n",
      "20/06/25 00:33:26 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "20/06/25 00:33:26 INFO PythonRunner: Times: total = 162, boot = 3, init = 50, finish = 109\n",
      "20/06/25 00:33:26 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:26 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 168 ms on localhost (executor driver) (7/10)\n",
      "20/06/25 00:33:26 INFO PythonRunner: Times: total = 143, boot = 3, init = 45, finish = 95\n",
      "20/06/25 00:33:26 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:26 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 150 ms on localhost (executor driver) (8/10)\n",
      "20/06/25 00:33:26 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
      "20/06/25 00:33:27 INFO PythonRunner: Times: total = 158, boot = 6, init = 46, finish = 106\n",
      "20/06/25 00:33:27 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1421 bytes result sent to driver\n",
      "20/06/25 00:33:27 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 7852 bytes)\n",
      "20/06/25 00:33:27 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
      "20/06/25 00:33:27 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 165 ms on localhost (executor driver) (9/10)\n",
      "20/06/25 00:33:27 INFO PythonRunner: Times: total = 151, boot = 4, init = 45, finish = 102\n",
      "20/06/25 00:33:27 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1378 bytes result sent to driver\n",
      "20/06/25 00:33:27 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 161 ms on localhost (executor driver) (10/10)\n",
      "20/06/25 00:33:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/06/25 00:33:27 INFO DAGScheduler: ResultStage 0 (reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43) finished in 2.373 s\n",
      "20/06/25 00:33:27 INFO DAGScheduler: Job 0 finished: reduce at /home/raj/online-courses/pyspark/Spark-The-Definitive-Guide/Notebooks/../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py:43, took 2.441946 s\n",
      "Pi is roughly 3.142600\n",
      "20/06/25 00:33:27 INFO SparkUI: Stopped Spark web UI at http://192.168.1.118:4040\n",
      "20/06/25 00:33:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/06/25 00:33:27 INFO MemoryStore: MemoryStore cleared\n",
      "20/06/25 00:33:27 INFO BlockManager: BlockManager stopped\n",
      "20/06/25 00:33:27 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/06/25 00:33:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/06/25 00:33:27 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/06/25 00:33:28 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/06/25 00:33:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbed8edd-b907-4696-86d2-3926f24e82e4/pyspark-7677be89-73f3-4f2d-b7e5-09a5bb1bd2f3\n",
      "20/06/25 00:33:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbed8edd-b907-4696-86d2-3926f24e82e4\n",
      "20/06/25 00:33:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2d17421-7871-405f-989d-57744de51391\n"
     ]
    }
   ],
   "source": [
    "! spark-submit --master local ../../spark-2.1.1-bin-hadoop2.7/examples/src/main/python/pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: Type-Safe Structured APIs\n",
    "+ The Dataset API is used for writing statically typed code (that does type checking at compile time as opposed to runtime) in Java and Scala.\n",
    "+ __The Dataset API is not available in Python and R, because those languages are dynamically typed.__\n",
    "+ DataFrames are a distributed collection of objects of type Row that can hold various types of tabular data. The Dataset API gives users the ability to assign a Java/Scala class to the records within a DataFrame and manipulate it as a collection of typed objects, similar to a Java _ArrayList_ or Scala _Seq_.\n",
    "+ The APIs available on Datasets are __type-safe__, meaning that we cannot accidentally view the objects in a Dataset as being of another class than the class you put in initially.\n",
    "+ With it, we can define our own data type and manipulate it via arbitrary functions. After we've performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreds of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming\n",
    "+ Structured Steaming is a high-level API for steam processing.\n",
    "+ With Structured Steaming, we can take the same operations that we perform in batch mode using Spark's structured APIs and run them in streaming fachion.\n",
    "+ This can reduce latency and allow for incremental processing.\n",
    "+ It allows us to rapidly and quickly extract value out of straming systems with virtually no code changes.\n",
    "+ It also makes it easy to conceptualize because we can write our batch job as a way to prototype it and then convert it to a streaming job.\n",
    "+ The way all of this works is by incrementally processing that data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"chapter3\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_dataset_path = \"../data/retail-data/by-day/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(retail_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staticDataFrame = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(retail_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   14075.0|[2011-12-05 05:45...|316.78000000000003|\n",
      "|   18180.0|[2011-12-05 05:45...|            310.73|\n",
      "|   15358.0|[2011-12-05 05:45...| 830.0600000000003|\n",
      "|   15392.0|[2011-12-05 05:45...|304.40999999999997|\n",
      "|   15290.0|[2011-12-05 05:45...|263.02000000000004|\n",
      "|   16811.0|[2011-12-05 05:45...|             232.3|\n",
      "|   12748.0|[2011-12-05 05:45...| 363.7899999999999|\n",
      "|   16500.0|[2011-12-05 05:45...| 52.74000000000001|\n",
      "|   16873.0|[2011-12-05 05:45...|1854.8300000000002|\n",
      "|   14060.0|[2011-12-05 05:45...|297.47999999999996|\n",
      "|   14649.0|[2011-12-05 05:45...| 513.9899999999998|\n",
      "|   16904.0|[2011-12-05 05:45...| 349.0200000000001|\n",
      "|   17857.0|[2011-12-05 05:45...|            2979.6|\n",
      "|   14083.0|[2011-12-05 05:45...| 446.5700000000001|\n",
      "|   14777.0|[2011-12-05 05:45...|             -2.95|\n",
      "|   16684.0|[2011-12-05 05:45...| 5401.979999999999|\n",
      "|   13685.0|[2011-12-05 05:45...|              5.48|\n",
      "|   15159.0|[2011-12-05 05:45...|           1730.84|\n",
      "|   18015.0|[2011-12-05 05:45...|            120.03|\n",
      "|   13305.0|[2011-12-05 05:45...|213.15999999999997|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col, dayofmonth, month\n",
    "staticDataFrame\\\n",
    ".selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice*Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "    col(\"CustomerId\"),\n",
    "    window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"with tmp as (select CustomerId, (UnitPrice*Quantity) as total_cost, InvoiceDate from retail_data) select CustomerId, sum(total_cost) as total_cost from tmp group by CustomerId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming code\n",
    "+ Changes\n",
    "    + Use __readStream__ instead of read\n",
    "    + __maxFilesPerTrigger__ option which simply specifies the number of files we should read in at once.(This is to make demonstration more \"streaming\" and in a production scenarios this woul probably be omitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    ".schema(staticSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(retail_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see whether our DataFrame is streaming using __.isStreaming__ param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame.\\\n",
    "selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice*Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "    col(\"CustomerId\"),\n",
    "    window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This is still a lazy operation, so we will need to call a streaming action to start the execution of this data flow.\n",
    "+ Sreaming actions are a bit different form our conventional static action because we're going to be populating data somewhere instead of just calling something like count(which doesn't make any sense on a stream anyways.\n",
    "+ __The action we will use will output to an in-memory table that we will update after each trigger. In this case, each trigger is based on an individual file ( the read option that we set).__\n",
    "+ __Spark will mutate the data in the in-memory table such that we will always have the highest value as specified in our previous aggregation.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f0894d2be10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we start the steam, we can run queries against it to debug what our result will look lik if we were to write this out to a production sink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   15671.0|[2011-04-11 05:45...|375.96000000000004|\n",
      "|   17576.0|[2010-12-13 05:45...|177.35000000000002|\n",
      "|   13240.0|[2011-03-27 05:45...|218.33999999999997|\n",
      "|   14911.0|[2011-03-11 05:45...|               0.0|\n",
      "|   13668.0|[2011-04-11 05:45...|            132.44|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY 'sum(total_cost)' DESC\n",
    "\"\"\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll notice that the composition of our table changes as we read in more data With each file, the results might or might not be changing based on the data.  \n",
    "Another option is to write the results out to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f0894d3f0b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"console\")\\\n",
    ".queryName(\"customer_purchases_2\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice how this window is built on event time, as well, not th time at which Spark process the data. This was one of the shortcomings of Spark Sreaming that Structured Sreaming has resolved__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning and Advanced Analytics\n",
    "&rightarrow; Another popular aspect of Spark is its ability to perform large-scale machine learning with a built-in library of machine algorithms called __MLlib__.\n",
    "+ MLlib allows for preprocessing , munging, training of models, and make predictions at scale on data.\n",
    "+ Models trained in MLlib can be used to make predictions in Structured Streaming\n",
    "+ Spark provides a sophisticated machine learning API for performaing a variety of machine learning tasks, from classification to regression, and clustering to deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "&rightarrow; Machine learning algorithms in MLlib require that data is represented as numberical values. We need to transform data of other types into some numerical representation for which we will use several DataFrame transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = staticDataFrame\\\n",
    ".na.fill(0)\\\n",
    ".withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"),\"EEEE\"))\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data set based on InvoiceDate\n",
    "trainDataFrame = preppedDataFrame\\\n",
    ".where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    ".where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245903"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataFrame.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StringIndexer\n",
    "&rightarrow; Spark's MLlib provides a number of transformations, with which we can automate some of our general transformations. One of such transformer is a __StringIndexer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    ".setInputCol(\"day_of_week\")\\\n",
    ".setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will trun our days of weeks into corresponding numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol(\"day_of_week_index\")\\\n",
    ".setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these will result in a set of columns that we will \"assemble\" into a vector.  \n",
    "__All machine learning algorithms in Spark take as input a Vector type, which must be a set of numerical values:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler()\\\n",
    ".setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have three key features: the price, the quantity, and the day of week. Next we will set this up into a __pipeline so that any future data we need to transform can go through the same process__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    ".setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing for training is a two-step process:  \n",
    "+ First, we need to fit our transformers to this dataset; they need to know the uniques values present to be indexed.\n",
    "+ After that, Spark can encode the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we fit the training data, we are ready to take that fitted pipeline and use it to transform all of our data in a consistent and repeatable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have included our model training in our pipeline.  \n",
    "But for the hyperparameter tunning on the model we don't do because we don't want to repeat the exact same transformations over and over again.  \n",
    "We'll use __caching__, an optimization which will put a copy of the intermediately transformed dataset into memory, allowing us to repeatedly access it at much lower cost than running the entire pipeline again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    ".setK(20)\\\n",
    ".setSeed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLib's DataFrame API follow the naming pattern of\n",
    "+ __Algorithm__ for the unntrained version like __KMeans__\n",
    "+ __AlgorithmModel__ for the trained version like __KMeansModel__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 1 kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "cachedkmeans = KMeans()\\\n",
    ".setK(20)\\\n",
    ".setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.87 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 1 cachedkmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedkmModel = cachedkmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6842576726028763"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "predictions = kmModel.transform(transformedTraining)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5427938390491535"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "predictions = kmModel.transform(transformedTest)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower-Level APIs \n",
    "+ Spark includes a number of lower-level primitives to allow for arbitrary Java and Python object manipulation via Resilient Distributed Datasets(RDDs). \n",
    "+ Virtually everything in Spark is built on top of RDDs.\n",
    "+ DataFrame operations are built on top of RDDs and compile down to these lower-level tools for convenient and extremely efficient distributed execution.\n",
    "+ There are some things for which we might use RDDs, __especially when we're reading or manipulating raw data__, but for most part we should stick to the __Structured APIs__.\n",
    "+ RDDs are lower level than DataFrames because they reveal physical execution characteristics (like partitions) to end users.\n",
    "+ __One thing that we might use RDDs for is to parallelize raw data that we have stored in memory on the driver machine.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
