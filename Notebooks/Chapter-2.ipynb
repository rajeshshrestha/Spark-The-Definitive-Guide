{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A gentle introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each language API maintains the same core concepts that we described earlier. There is a __SparkSession__ object available to the user, which is the entrance point to running Spark code. When using Spark from Python or R, JVM instructions aren't written explicity. The python, or R code is translated by Spark which can be run on the ececutor JVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: We can start interactive shell using pyspark but there is also a process of rsubmitting standalone applications to Spark called _spark-submit_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ User controls Spark Application through a driver process called the Spark Session\n",
    "+ The SparkSession instance is the way Spark executes user-defined manipulations across the cluster\n",
    "+ __There is a one-to-one correspondence between a SparkSession and a Spark Application__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"introduction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.118:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>introduction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f02bd9fa278>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe with one column containing 1000 rows with values from 0 to 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange = spark.range(1000)\n",
    "myRange.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange = myRange.toDF(\"number\") # returns a new dataframe with specified column name\n",
    "myRange.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Structured API and simply represents a table of data with rows and columns\n",
    "+ The list that defines the columns and the types within those columns is called the _schema-\n",
    "+ __Difference with spreadsheet__: a spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span on thousands of computers. \n",
    "+ __Reason for putting data on multiple computers__: Either data is too large to fit or it would take too long to perform computation on it on a single machine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets(RDDs). These different abstractions all represent distributed collections of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ To allow every executor to perform work in parallel, Spark breaks up the data into chunks called __partition__.\n",
    "+ A partition is a collection of rows that sit on one physical machine in acluster\n",
    "+ A DataFrame's partitions represent how the data is physically distirbuted across the cluster of machines during execution\n",
    "+ __For parallelism, both multiple partitions and multiple executors are needed__\n",
    "+ __For DataFrame, we do not (for the most part) manipulate partitions manually or individually. We simply specify high-level transformations of data in the physical partitions, and Spark determines how this work will actually execute on the cluster.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In Spark, the core data structures are immutable\n",
    "+ To \"change\" a DataFrame, we need to instruct Spark how it should be modified which are called __transformations__\n",
    "+ __NOTE:__ Spark will not act on transformations until we call an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\") # Doesn't return output. This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action\n",
    "divisBy2.show() # only executed here when show action is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.filter(\"number % 2 = 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divisBy2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Transormations Types:\n",
    "    + __Those that specify narrow dependencies__: For these, each input partition will contribute to only one output partition. Above example of \"number % 2 = 0) is an example of narrow transformations\n",
    "    + __Those that specify wide dependencies__: These will have input partitions contributing to many ouput partitions. This is often referred to as a __shuffle__ whereby Spark will exchange partiions across the cluster.\n",
    "+ __NOTE:__ With narrow transformations, Spark will automatically perform and operation called __piplining__, meaning that if we specify multiple filters on DataFrames, they'll all be performed in-memory. The same cannot be said for shuffles. When a shuffle is performed, Spark writes the results to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation \n",
    "\n",
    "+ It means that Spark will wait until the very last moment to execute the graph of computation instructions. In Spark, instead of modifying the data immediately when we express some operation, we build up a plan of transformations that you would like to apply to source data.\n",
    "+ By waiting until the last minute to execute the code, Spark compiles this paln from raw DataFrame transformations to a streamlined physical plan that will run as efficiently as possible across the cluster.\n",
    "+ __This provides immense benefits because Spark can optimize the entire data flow from end to end.__\n",
    "\n",
    "__Example Predicate Pushdown:__ If we build a large Spark job but specify a filter at the end that requires us to fetch one row from our source data, the most efficient way to execute this is to access the single record that we need. Spark will actually optimize this for us by pushning the filter down automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "+ Transformation allow us to build up our logical transformation plan. __To trigger the computation we run an action which instructs Spark to compute a result from a series of transformation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count() # returns the number of rows which is an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of action\n",
    "+ Actions to view data in the console\n",
    "+ Actions to collect data to native objects in the respective language\n",
    "+ Actions to write to output data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UI\n",
    "+ Runs at 4040 port of the driver node\n",
    "+ Runs at _http://localhost:4040_ if running in local mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An End-to-End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This DataFrame have a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take action\n",
    "![image.png](Images/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Sort</ins>\n",
    "+ It does not modify the DataFrame. It returns a new DataFrame by transforming the previous DataFrame\n",
    "+ Nothing happens to the data when we call sort because it's just a transformation.\n",
    "+ We can see that Spark is building up a plan for how it will execute this across the cluster by looking at the __explain__ plan.\n",
    "![image.png](Images/6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#576 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#576 ASC NULLS FIRST, 5), true, [id=#1207]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#574,ORIGIN_COUNTRY_NAME#575,count#576] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/raj/online-courses/pyspark/spark_the_definitive_guide/Spark-The-Defi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We can read plans from top to bottom, __the top being the end result, and the bottom bein gthe source(s) of data.__\n",
    "+ By default, when we perform a shuffle __(wide_transformation)__, Sparks outputs __200 shuffle partitions.__  \n",
    "  \n",
    "We can change this by setting a configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#576 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#576 ASC NULLS FIRST, 5), true, [id=#1219]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#574,ORIGIN_COUNTRY_NAME#575,count#576] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/raj/online-courses/pyspark/spark_the_definitive_guide/Spark-The-Defi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain() \n",
    "# partitions changed to 5 unlike 200 in previous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Above whole process</ins>\n",
    "__The data is partitioned on wide transformation(shuffle) which is sort.__\n",
    "![image.png](Images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames and SQL\n",
    "+ We can specify logic in SQL or DataFrames and Spark will comple that logic down to an underlying plan before actually executing code\n",
    "+ With Spark SQL, we can __register any DataFrame as a table or view (a temporary table) and query it using pure SQL__\n",
    "+ There is no performance difference between writing SQL queries or writing DataFrame code, they both \"compile\" to the same underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Create Table or view</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Query data in SQL</ins>\n",
    "+ We can use spark.sql function that returns a new DataFrame\n",
    "+ A SQL query against a DataFrame returns another DataFrame which is actually quite powerful\n",
    "+ __This makes it possible to specify transformations in the manner most convenient to the user at any given point in time and not sacrifice any efficiency to do so__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "select DEST_COUNTRY_NAME, count(1)\n",
    "from flight_data_2015\n",
    "group by DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "#COUNT(1) is basically just counting a constant value 1 column for each row\n",
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#574], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#574, 5), true, [id=#1248]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#574], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#574] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/raj/online-courses/pyspark/spark_the_definitive_guide/Spark-The-Defi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#574], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#574, 5), true, [id=#1267]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#574], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#574] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/raj/online-courses/pyspark/spark_the_definitive_guide/Spark-The-Defi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|             Moldova|       1|\n",
      "|             Bolivia|       1|\n",
      "|             Algeria|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|            Pakistan|       1|\n",
      "|    Marshall Islands|       1|\n",
      "|            Suriname|       1|\n",
      "|              Panama|       1|\n",
      "|         New Zealand|       1|\n",
      "|             Liberia|       1|\n",
      "|             Ireland|       1|\n",
      "|              Zambia|       1|\n",
      "|            Malaysia|       1|\n",
      "|               Japan|       1|\n",
      "|    French Polynesia|       1|\n",
      "|           Singapore|       1|\n",
      "|             Denmark|       1|\n",
      "|               Spain|       1|\n",
      "|             Bermuda|       1|\n",
      "|            Kiribati|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|count|\n",
      "+--------------------+-----+\n",
      "|             Moldova|    1|\n",
      "|             Bolivia|    1|\n",
      "|             Algeria|    1|\n",
      "|Turks and Caicos ...|    1|\n",
      "|            Pakistan|    1|\n",
      "|    Marshall Islands|    1|\n",
      "|            Suriname|    1|\n",
      "|              Panama|    1|\n",
      "|         New Zealand|    1|\n",
      "|             Liberia|    1|\n",
      "|             Ireland|    1|\n",
      "|              Zambia|    1|\n",
      "|            Malaysia|    1|\n",
      "|               Japan|    1|\n",
      "|    French Polynesia|    1|\n",
      "|           Singapore|    1|\n",
      "|             Denmark|    1|\n",
      "|               Spain|    1|\n",
      "|             Bermuda|    1|\n",
      "|            Kiribati|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select max(count)\n",
    "from flight_data_2015\"\"\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count=370002)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max('count').alias(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding top 5 countries\n",
    "# SQL Code\n",
    "maxSql = spark.sql(\"\"\"\n",
    "select DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "from flight_data_2015\n",
    "group by DEST_COUNTRY_NAME\n",
    "order by destination_total desc\n",
    "limit 5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding top 5 countries\n",
    "# DataFrame code\n",
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    ".groupby(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The execution plan is a directed acyclic graph(DAG) of transformations, each resulting in a new immutable DataFrame, on which we call an action to generate a result.__\n",
    "![image.png](Images/8.png)\n",
    "  \n",
    "    \n",
    "    \n",
    "+ __The first step is to read in the data. We defined the DataFrame previously but, as a reminder, Spark does not actually read it in until an action is calle don that DataFrame or one derived from the original DataFrame__\n",
    "+ In general, may DataFRame methods will accept strings (as column names) or Column types or expressions. Columns and expressions are actually the exact same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#684L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#574,destination_total#684L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#574], functions=[sum(cast(count#576 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#574, 5), true, [id=#1462]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#574], functions=[partial_sum(cast(count#576 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#574,count#576] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/raj/online-courses/pyspark/spark_the_definitive_guide/Spark-The-Defi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    ".groupby(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
